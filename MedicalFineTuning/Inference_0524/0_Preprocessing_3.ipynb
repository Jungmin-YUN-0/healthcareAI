{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woogoonkyu/anaconda3/envs/lora_null/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import default_data_collator, DataCollatorForLanguageModeling, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoConfig\n",
    "from torch.optim import AdamW  # ✅ 이걸로 교체\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 셋 구성\n",
    "\n",
    "- 1) saltlux/Ko-Llama3-Luxia-8B\n",
    "- 2) beomi/Llama-3-KoEn-8B-Instruct-preview \n",
    "- 3) beomi/Llama-3-Open-Ko-8B\n",
    "- 4) MLP-KTLim/llama-3-Korean-Bllossom-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = 'saltlux/Ko-Llama3-Luxia-8B'\n",
    "model_name2 = 'beomi/Llama-3-KoEn-8B-Instruct-preview'\n",
    "model_name3 = 'beomi/Llama-3-Open-Ko-8B'\n",
    "model_name4 = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name3)\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(model_name4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "알겠습니다. 그리고 소변이 느리게 흐르고, 물이 새어나오는 문제는 어떤 상황에서 더 심해지나요?\n",
      "특별한 상황은 없고, 그냥 일상적으로 느린 것 같아요.\n",
      "\n",
      "***Answer:\n"
     ]
    }
   ],
   "source": [
    "def transform_prompt(prompt: str) -> str:\n",
    "    # 추가할 지침 문장\n",
    "    role_instruction = \"You will always respond as the doctor, and your answer must be written in Korean only.\\n\\n\"\n",
    "\n",
    "    # 1. '***Conversation:' 위치 찾기\n",
    "    parts = prompt.split('***Conversation:')\n",
    "    if len(parts) != 2:\n",
    "        return prompt  # 변환할 수 없는 구조는 그대로 반환\n",
    "\n",
    "    header, conversation = parts\n",
    "\n",
    "    # 2. Client:, Doctor: 라벨 제거\n",
    "    conversation_cleaned = re.sub(r'^(Client|Doctor):\\s*', '', conversation, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. 역할 문장을 Conversation 위에 삽입\n",
    "    transformed_prompt = header.strip() + \"\\n\\n\" + role_instruction + \"***Conversation:\" + conversation_cleaned\n",
    "\n",
    "    return transformed_prompt\n",
    "\n",
    "# 예시\n",
    "example = \"\"\"You are a medical professional proficient in both Korean and English.\n",
    "Your task is to carefully read the conversation between a Client and a Doctor, \n",
    "then predict the next response from the Doctor in a professional and empathetic manner.\n",
    "\n",
    "Please respond in **Korean** only.\n",
    "\n",
    "Now produce the next Doctor response to the Client’s last question.\n",
    "Return your answer in Korean.\n",
    "\n",
    "***Conversation:\n",
    "Doctor: 알겠습니다. 그리고 소변이 느리게 흐르고, 물이 새어나오는 문제는 어떤 상황에서 더 심해지나요?\n",
    "Client: 특별한 상황은 없고, 그냥 일상적으로 느린 것 같아요.\n",
    "\n",
    "***Answer:\"\"\"\n",
    "\n",
    "# 변환 실행\n",
    "transformed = transform_prompt(example)\n",
    "print(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 15475/15475 [00:00<00:00, 286007.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3870/3870 [00:00<00:00, 281966.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk('combined_dataset_short')\n",
    "def truncate_after_answer(example):\n",
    "    if example['source'] == 'dialogue':\n",
    "        example['input'] = transform_prompt(example['input'])\n",
    "        example['output'] = re.sub(r'^(Client|Doctor):\\s*', '', example['output'], flags=re.MULTILINE)\n",
    "\n",
    "    if example['source'] == 'qa':\n",
    "        example['input'] = example['input'].replace('Choice','선택')\n",
    "        example['input'] = example['input'].replace('Answer:','***Answer:').replace('Question:','***Question:' )\n",
    "        example['input'] = example['input'].strip()\n",
    "        example['output'] = example['output'].replace('Choice','선택').strip()\n",
    "        \n",
    "    if '***Answer:' in example['input']:\n",
    "        # '***Answer:'까지만 남기고 뒷부분 삭제\n",
    "        idx = example['input'].index('***Answer:') + len('***Answer:')\n",
    "        example['input'] = example['input'][:idx].strip()\n",
    "    return example\n",
    "\n",
    "# 적용\n",
    "train_cleaned = dataset['train'].map(truncate_after_answer)\n",
    "test_cleaned = dataset['test'].map(truncate_after_answer)\n",
    "\n",
    "# 다시 DatasetDict로\n",
    "cleaned_dataset = DatasetDict({\n",
    "    'train': train_cleaned,\n",
    "    'test': test_cleaned\n",
    "})\n",
    "\n",
    "\n",
    "cleaned_dataset.save_to_disk(\"cleaned_dataset_with_answer_tag_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = load_from_disk('cleaned_dataset_with_answer_tag_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dialogue', 'qa')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cleaned_dataset['train']['source'][0], cleaned_dataset['test']['source'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dialogue', 'qa')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset['test']['source'][0], cleaned_dataset['test']['source'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "간전문의를 받으려면 어떻게 해야 하나요?\n",
      "저희 병원에서 간전문의를 받을 수 있도록 예약을 도와드리겠습니다. 또한, 음주 중단과 혈당 관리에 대한 교육도 제공해드릴 수 있습니다.\n",
      "감사합니다. 얼마나 빨리 예약을 잡아야 하나요?\n",
      "\n",
      "***Answer:\n",
      "##################################################\n",
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "소변을 보내기 전에 조금 힘들어요.\n",
      "알겠습니다. 그리고 소변이 느리게 흐르고, 물이 새어나오는 문제는 어떤 상황에서 더 심해지나요?\n",
      "특별한 상황은 없고, 그냥 일상적으로 느린 것 같아요.\n",
      "\n",
      "***Answer:\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dataset['train']['input'][0])\n",
    "print ('#' * 50)\n",
    "print(cleaned_dataset['train']['input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가능한 빨리 예약을 잡아드리겠습니다. 환자님의 상태를 고려하여 적절한 시기에 예약이 이루어질 수 있도록 도와드리겠습니다.\n",
      "##################################################\n",
      "혹시 배뇨 중에 통증이나 불편함을 느끼시나요?\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dataset['train']['output'][0])\n",
    "print ('#' * 50)\n",
    "print(cleaned_dataset['train']['output'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "간전문의를 받으려면 어떻게 해야 하나요?\n",
      "저희 병원에서 간전문의를 받을 수 있도록 예약을 도와드리겠습니다. 또한, 음주 중단과 혈당 관리에 대한 교육도 제공해드릴 수 있습니다.\n",
      "감사합니다. 얼마나 빨리 예약을 잡아야 하나요?\n",
      "\n",
      "***Answer:\n",
      "##################################################\n",
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "소변을 보내기 전에 조금 힘들어요.\n",
      "알겠습니다. 그리고 소변이 느리게 흐르고, 물이 새어나오는 문제는 어떤 상황에서 더 심해지나요?\n",
      "특별한 상황은 없고, 그냥 일상적으로 느린 것 같아요.\n",
      "\n",
      "***Answer:\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dataset['train']['input'][0])\n",
    "print ('#' * 50)\n",
    "print(cleaned_dataset['train']['input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택 5, 환자의 병력조사 후 급성 증상을 완화해준다.\n",
      "##################################################\n",
      "선택 3, 졸레드론산(zoledronic acid)\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dataset['test']['output'][-1])\n",
    "print ('#' * 50)\n",
    "print(cleaned_dataset['test']['output'][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English. \n",
      "Your task is to carefully analyze a given medical question and its five answer choices, \n",
      "then provide the correct answer.\n",
      "The answer choice will always follow the fixed format of 선택 1, 선택 2, 선택 3, 선택 4, 선택 5.\n",
      "Return the response in the following format:\n",
      "선택 answer\n",
      "\n",
      "Now, analyze the following medical question and return the answer.\n",
      "\n",
      "***Question: \"갑자기 잇몸 전체가 아파요.\"라며 내원한 20세 여자에 대한 임상소견은 다음과 같다.\n",
      "심한 구취\n",
      "불량한 구강위생\n",
      "자발적 치은출혈\n",
      "변연치은에 분화구 형성\n",
      "치간유두의 괴사성 형태\n",
      "가장 먼저 할 처치는?\n",
      "선택 1: 마취 후 생검을 시행한다.\n",
      "선택 2: 치은연상 치석제거를 시행한다.\n",
      "선택 3: 치은연하 육아조직제거를 시행한다.\n",
      "선택 4: 치면세마 후 구강위생상태를 평가한다.\n",
      "선택 5: 환자의 병력조사 후 급성 증상을 완화해준다.\n",
      "***Answer:\n",
      "##################################################\n",
      "You are a medical professional proficient in both Korean and English. \n",
      "Your task is to carefully analyze a given medical question and its five answer choices, \n",
      "then provide the correct answer.\n",
      "The answer choice will always follow the fixed format of 선택 1, 선택 2, 선택 3, 선택 4, 선택 5.\n",
      "Return the response in the following format:\n",
      "선택 answer\n",
      "\n",
      "Now, analyze the following medical question and return the answer.\n",
      "\n",
      "***Question: 다음 사례를 읽고 각 문제에 적합한 답을 고르시오. 심부정맥혈전증 병력이 있는 68세 여자가 류마티스관절염과 골다공증 치료 중, 최근 관절의 부종과 통증이 심해져 에타너셉트(etanercept) 주사제를 추가하려고 한다.[복용약물] 메토트렉세이트(methotrexate) 폴산(folic acid) 히드록시클로로퀸(hydroxychloroquine) 리세드론산(risedronate) 이 환자에게 약물로 조절되지 않는 위식도역류질환이 발생하여 골다공증 치료약을 변경하고자 한다. 적절한 약물은?\n",
      "선택 1: 라록시펜(raloxifene)\n",
      "선택 2: 알렌드론산(alendronate)\n",
      "선택 3: 졸레드론산(zoledronic acid)\n",
      "선택 4: 바제독시펜/에스트로겐(bazedoxifene/conjugated equine estrogen)\n",
      "선택 5: 에스트라디올/메드록시프로게스테론(estradiol/medroxyprogesterone)\n",
      "***Answer:\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dataset['test']['input'][-1])\n",
    "print ('#' * 50)\n",
    "print(cleaned_dataset['test']['input'][-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing QA & Generation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('/nas_homes/projects/rapa/Dataset/data_updated_512_instruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8327\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# 예시 tokenizer (사용 중인 모델에 맞게 수정하세요)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-KoEn-8B-Instruct-preview\")  # 또는 llama2, mistral 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8327/8327 [00:27<00:00, 303.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0  Please read the following content and respond ...   \n",
      "1  Please read the following content and respond ...   \n",
      "2  Please read the following content and respond ...   \n",
      "3  Please read the following content and respond ...   \n",
      "4  Please read the following content and respond ...   \n",
      "\n",
      "                                          label_text  \n",
      "0  환자의 징후와 증상, 부검 시 대뇌 피질에 루이체 소견이 있는 것으로 보아 루이체 ...  \n",
      "1  당뇨병은 당뇨병성 신증으로 알려진 과정을 통해 만성 신장 질환으로 이어질 수 있습니...  \n",
      "2  Title: 당뇨병 관리 - 새로 진단받은 환자를 위한 안내서\\n\\n소개:\\n당뇨병...  \n",
      "3  내분비계는 수많은 신체 기능을 조절하는 화학적 메신저인 호르몬을 생산하고 분비하는 ...  \n",
      "4  폐암을 유발하는 위험 요인을 줄이려면 다음 단계를 수행할 수 있습니다:\\n\\n1. ...  \n"
     ]
    }
   ],
   "source": [
    "def decode_batch(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    label_ids = [id for id in example[\"labels\"] if id != -100]  # <- 여기 핵심!\n",
    "\n",
    "    decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    decoded_label = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\"decoded_input\": decoded_input, \"decoded_label\": decoded_label}\n",
    "\n",
    "decoded_dataset = dataset['test'].map(decode_batch)\n",
    "df = decoded_dataset.to_pandas()[[\"decoded_input\", \"decoded_label\"]]\n",
    "df.rename(columns={\"decoded_input\": \"input_text\", \"decoded_label\": \"label_text\"}, inplace=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('COT_data.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Data with Mode Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import random\n",
    "\n",
    "# 모델에 맞는 tokenizer 로딩\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = tokenizer1\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 데이터셋 로딩\n",
    "dataset = load_from_disk(\"cleaned_dataset_with_answer_tag_new\")\n",
    "\n",
    "def preprocess_function(example):\n",
    "    prompt = example[\"input\"]\n",
    "    answer = example[\"output\"]\n",
    "\n",
    "    # \"CLM 방식\"에 맞게 전체 시퀀스를 하나로 연결\n",
    "    full_text = prompt + \" \" + answer\n",
    "    answer_tag = \"***Answer:\"\n",
    "\n",
    "    # label을 마스킹하기 위한 기준점 위치\n",
    "    answer_start = prompt.find(answer_tag)\n",
    "    if answer_start == -1:\n",
    "        if \"Answer:\" in prompt:\n",
    "            prompt = prompt.replace('Answer:', \"***Answer:\")\n",
    "        else:\n",
    "            print(prompt)\n",
    "            raise ValueError(\"prompt 안에 ***Answer: 태그가 없음\")\n",
    "        \n",
    "\n",
    "    # full_text 전체를 토크나이즈\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # label을 생성할 때, answer 시작 전까지는 -100\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "    labels = tokenized[\"input_ids\"]\n",
    "    masked_labels = [\n",
    "        label if idx >= len(prompt_tokenized[\"input_ids\"]) and label != tokenizer.pad_token_id else -100\n",
    "        for idx, label in enumerate(labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(tokenized[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(tokenized[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(masked_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beomi'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.name_or_path.split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saltlux/Ko-Llama3-Luxia-8B',\n",
       " 'beomi/Llama-3-KoEn-8B-Instruct-preview',\n",
       " 'beomi/Llama-3-Open-Ko-8B',\n",
       " 'MLP-KTLim/llama-3-Korean-Bllossom-8B')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.name_or_path, tokenizer2.name_or_path, tokenizer3.name_or_path, tokenizer4.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 15475/15475 [00:28<00:00, 546.32 examples/s]\n",
      "Map: 100%|██████████| 3870/3870 [00:07<00:00, 551.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 15475/15475 [00:00<00:00, 145692.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3870/3870 [00:00<00:00, 133758.18 examples/s]\n",
      "100%|██████████| 1/1 [00:35<00:00, 35.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num = 0\n",
    "name_lst = []\n",
    "\n",
    "\n",
    "model_name2 = 'beomi/Llama-3-KoEn-8B-Instruct-preview'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "\n",
    "for i in tqdm([tokenizer2]):\n",
    "\n",
    "    tokenizer = i\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    name = tokenizer.name_or_path.split('/')[0].replace('-','_')\n",
    "\n",
    "    # 데이터셋 전체에 전처리 적용\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "    # 원본 텍스트 열 제거\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"input\", \"output\", \"source\"])\n",
    "\n",
    "    # 디스크에 저장\n",
    "    if name in name_lst:\n",
    "        name = name + str(num)\n",
    "        num += 1\n",
    "    else:\n",
    "        pass\n",
    "    tokenized_datasets.save_to_disk(f\"tokenizer_dataset_shorts_{name}\")\n",
    "    name_lst.append(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n",
      "Next name : beomi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English.\n",
      "Your task is to carefully read the conversation between a Client and a Doctor, \n",
      "then predict the next response from the Doctor in a professional and empathetic manner.\n",
      "\n",
      "Please respond in **Korean** only.\n",
      "\n",
      "Now produce the next Doctor response to the Client’s last question.\n",
      "Return your answer in Korean.\n",
      "\n",
      "You will always respond as the doctor, and your answer must be written in Korean only.\n",
      "\n",
      "***Conversation:\n",
      "그럼 어떤 검사를 받아야 할까요? 그리고 어떻게 대처해야 할까요?\n",
      "Chrystal 씨, 현재 증상과 과거 병력을 고려하여 복부 초음파 검사와 혈액 검사를 권장합니다. 이를 통해 감염의 정도와 기타 가능한 원인을 확인할 수 있습니다. 대처 방법은 항생제 치료와 함께 적절한 통증 완화 조치를 취하는 것입니다. 또한, 현재 당뇨병을 가지고 계신데, 혈당 관리에도 신경써야 합니다.\n",
      "이해했습니다. 복부 초음파 검사와 혈액 검사를 받아보고 항생제와 통증 완화 조치를 취하겠습니다. 혈당 관리 역시 신경쓰겠습니다. 감사합니다.\n",
      "\n",
      "***Answer: 별 말씀을요, Chrystal 씨. 건강에 더욱 신경 쓰시고 복원되시기를 기원합니다. 궁금한 점이 있으시면 언제든지 물어보세요.\n",
      " 별 말씀을요, Chrystal 씨. 건강에 더욱 신경 쓰시고 복원되시기를 기원합니다. 궁금한 점이 있으시면 언제든지 물어보세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm([tokenizer2]):\n",
    "\n",
    "    print('#################################################')\n",
    "    tokenizer = i\n",
    "    name = tokenizer.name_or_path.split('/')[0].replace('-','_')\n",
    "    print(f'Next name : {name}')\n",
    "    \n",
    "\n",
    "\n",
    "    dataset = load_from_disk(f\"tokenizer_dataset_shorts_{name}\")\n",
    "\n",
    "    num = 3240\n",
    "\n",
    "    sample = dataset['train']['input_ids'][num]\n",
    "    sample_output = [i for i in dataset['train']['labels'][num] if i != -100]\n",
    "    print(tokenizer.decode(sample, skip_special_tokens= True))\n",
    "\n",
    "    print(tokenizer.decode(sample_output)) # PAD, EOS -100으로 다 설정했으므로 Special token 안나와야함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical professional proficient in both Korean and English. \n",
      "Your task is to carefully analyze a given medical question and its five answer choices, \n",
      "then provide the correct answer.\n",
      "The answer choice will always follow the fixed format of 선택 1, 선택 2, 선택 3, 선택 4, 선택 5.\n",
      "Return the response in the following format:\n",
      "선택 answer\n",
      "\n",
      "Now, analyze the following medical question and return the answer.\n",
      "\n",
      "***Question: \"갑자기 잇몸 전체가 아파요.\"라며 내원한 20세 여자에 대한 임상소견은 다음과 같다.\n",
      "심한 구취\n",
      "불량한 구강위생\n",
      "자발적 치은출혈\n",
      "변연치은에 분화구 형성\n",
      "치간유두의 괴사성 형태\n",
      "가장 먼저 할 처치는?\n",
      "선택 1: 마취 후 생검을 시행한다.\n",
      "선택 2: 치은연상 치석제거를 시행한다.\n",
      "선택 3: 치은연하 육아조직제거를 시행한다.\n",
      "선택 4: 치면세마 후 구강위생상태를 평가한다.\n",
      "선택 5: 환자의 병력조사 후 급성 증상을 완화해준다.\n",
      "***Answer: 선택 5, 환자의 병력조사 후 급성 증상을 완화해준다.\n",
      " 선택 5, 환자의 병력조사 후 급성 증상을 완화해준다.\n"
     ]
    }
   ],
   "source": [
    "name = ['saltlux','beomi_Koen_instruct','beomi0_Ko','MLP_KTLim']\n",
    "\n",
    "\n",
    "model_name2 = 'beomi/Llama-3-KoEn-8B-Instruct-preview'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_from_disk(f\"tokenizer_dataset_shorts_beomi\")\n",
    "\n",
    "num = -1\n",
    "\n",
    "sample = dataset['test']['input_ids'][num]\n",
    "sample_output = [i for i in dataset['test']['labels'][num] if i != -100]\n",
    "print(tokenizer.decode(sample, skip_special_tokens= True))\n",
    "\n",
    "print(tokenizer.decode(sample_output)) # PAD, EOS -100으로 다 설정했으므로 Special token 안나와야함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_null",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
