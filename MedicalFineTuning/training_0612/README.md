# LLM Fine-tuning with LoRA/QLoRA

λ€ν• μ–Έμ–΄ λ¨λΈ(LLM) νμΈνλ‹μ„ μ„ν• LoRA/QLoRA κΈ°λ° ν›λ ¨ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

---

## π”§ μ£Όμ” νΉμ§•

- **LoRA/QLoRA μ§€μ›**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
- **DeepSpeed ν†µν•©**: λ¶„μ‚° ν›λ ¨ μµμ ν™”  
- **4bit μ–‘μν™”**: QLoRAλ¥Ό ν†µν• λ©”λ¨λ¦¬ μ μ•½
- **μλ™ μ²΄ν¬ν¬μΈνΈ**: ν›λ ¨ μ¤‘λ‹¨ μ‹ μ¬μ‹μ‘ κ°€λ¥

---

## π“¦ μ”κµ¬μ‚¬ν•­

```bash
pip install torch transformers datasets peft accelerate deepspeed bitsandbytes wandb
```

---

## π“„ λ°μ΄ν„°μ…‹ ν•μ‹

λ°μ΄ν„°μ…‹μ€ λ‹¤μ ν•μ‹μ΄μ–΄μ•Ό ν•©λ‹λ‹¤:

```json
{
  "input": "μ§λ¬Έ ν…μ¤νΈ",
  "output": "λ‹µλ³€ ν…μ¤νΈ", 
  "source": "λ°μ΄ν„° μ¶μ² (μ„ νƒμ‚¬ν•­)"
}
```

---

## β™οΈ μ‚¬μ©λ²•

### 1. μ„¤μ • μμ •

`run.sh`μ—μ„ κ²½λ΅μ™€ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μμ •ν•μ„Έμ”:

```bash
# λ¨λΈ λ° λ°μ΄ν„° κ²½λ΅
BASE_MODEL_NAME="/path/to/your/base/model"
DATA_PATH="/path/to/your/dataset"
CHECKPOINT_PATH="/path/to/checkpoint"
BASE_SAVE_PATH="/path/to/save/model"

# ν•μ΄νΌνλΌλ―Έν„°
USE_QLORA="True"  # QLoRA μ‚¬μ© μ—¬λ¶€
NUM_EPOCHS=3      # ν›λ ¨ μ—ν­ μ
LORA_RANK=16      # LoRA λ­ν¬
```

---

### 2. ν›λ ¨ μ‹¤ν–‰

```bash
chmod +x run.sh
./run.sh
```

`run.sh` λ‚΄λ¶€μ—μ„ `train.py`κ°€ νΈμ¶λλ©°, μ•„λ κΈ°λ¥μ„ μν–‰ν•©λ‹λ‹¤:

#### π”Ή `train.py`: νμΈνλ‹ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ

- Hugging Faceμ `transformers` λ° `peft` λΌμ΄λΈλ¬λ¦¬λ¥Ό κΈ°λ°μΌλ΅ LoRA/QLoRA νμΈνλ‹
- `Trainer` λλ” `SFTTrainer` μ‚¬μ©
- QLoRA μ‚¬μ© μ‹ 4bit μ–‘μν™” μ μ©
- `wandb` μ—°λ™μ„ ν†µν• λ΅κΉ… μ§€μ›

```bash
python train.py \
    --base_model "/path/to/base_model" \
    --dataset_path "/path/to/dataset" \
    --output_dir "/path/to/save/output" \
    --use_qlora True \
    --num_epochs 3 \
    --lora_r 16 \
    --lora_alpha 32 \
    --learning_rate 1e-4
```

---

### 3. ν›λ ¨ μ¬μ‹μ‘ (μµμ…)

μ²΄ν¬ν¬μΈνΈμ—μ„ ν›λ ¨μ„ μ¬μ‹μ‘ν•λ ¤λ©΄:

```bash
# run.shμ—μ„ JOB λ³€μ μμ •
JOB="resume_training"
```

---

## π”— λ¨λΈ λ³‘ν•© λ° μ¶”λ΅  μ¤€λΉ„

ν›λ ¨μ΄ μ™„λ£λ ν›„ μ–΄λ‘ν„° λ¨λΈμ„ λ² μ΄μ¤ λ¨λΈκ³Ό λ³‘ν•©ν•μ—¬ μ¶”λ΅  μµμ ν™” λ¨λΈμ„ μƒμ„±ν•  μ μμµλ‹λ‹¤.

#### π”Ή `model_merge.py`: LoRA μ–΄λ‘ν„° λ³‘ν•© μ¤ν¬λ¦½νΈ

- LoRA/QLoRA μ–΄λ‘ν„°λ¥Ό base modelμ— λ³‘ν•©
- μµμΆ… λ¨λΈμ„ Hugging Face νΈν™ ν¬λ§·μΌλ΅ μ €μ¥

```bash
python model_merge.py \
    --base_model "/path/to/base_model" \
    --adapter_model "/path/to/adapter" \
    --output_dir "/path/to/merged_model"
```

---

## β™οΈ μ£Όμ” νλΌλ―Έν„°

| νλΌλ―Έν„°        | μ„¤λ…                           | κΈ°λ³Έκ°’                     |
|------------------|----------------------------------|-----------------------------|
| `USE_QLORA`      | QLoRA μ‚¬μ© μ—¬λ¶€                  | `True`                      |
| `NUM_EPOCHS`     | ν›λ ¨ μ—ν­ μ                     | `3`                         |
| `LORA_RANK`      | LoRA λ­ν¬ (λ¨λΈ λ³µμ΅λ„)         | `16`                        |
| `LORA_ALPHA`     | LoRA μ¤μΌ€μΌλ§ νλΌλ―Έν„°           | `32`                        |
| `LEARNING_RATE`  | ν•™μµλ¥                             | `1e-4` (QLoRA), `5e-6` (LoRA) |

---

## π§  λ©”λ¨λ¦¬ μµμ ν™” μ „λµ

- **QLoRA**: 4bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ†  
- **Gradient Checkpointing**: λ©”λ¨λ¦¬ β†” κ³„μ‚° νΈλ μ΄λ“μ¤ν”„  
- **DeepSpeed**: λ¶„μ‚° ν›λ ¨μΌλ΅ λ©”λ¨λ¦¬ λ¶„μ‚°  

---

## π“ λ¨λ‹ν„°λ§

ν›λ ¨ μ§„ν–‰ μƒν™©μ€ **Weights & Biases (wandb)** λ΅ λ¨λ‹ν„°λ§λ©λ‹λ‹¤:

- **ν”„λ΅μ νΈλ…**: `RAPA`  
- **μ‹¤ν–‰λ…**: `{λ¨λΈλ…}_{LoRAμ ν•}`  

---

## π“ μ¶λ ¥ κµ¬μ΅°

```plaintext
final_model/
β”β”€β”€ {model_name}/
β”‚   β””β”€β”€ {num_epochs}/
β”‚       β”β”€β”€ qlora_adapters/  # QLoRA μ–΄λ‘ν„°
β”‚       β””β”€β”€ qlora_merged/   # QLoRA λ³‘ν•©λ λ¨λΈ
β”‚       β””β”€β”€ lora_adapters/   # LoRA μ–΄λ‘ν„°
β”‚       β””β”€β”€ lora_merged/   # LoRA λ³‘ν•©λ λ¨λΈ

```
