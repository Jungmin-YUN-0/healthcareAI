# LLM Fine-tuning with LoRA/QLoRA

λ€ν• μ–Έμ–΄ λ¨λΈ(LLM) νμΈνλ‹μ„ μ„ν• LoRA/QLoRA κΈ°λ° ν›λ ¨ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

---

## π”§ μ£Όμ” νΉμ§•

- **LoRA/QLoRA μ§€μ›**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
- **DeepSpeed ν†µν•©**: λ¶„μ‚° ν›λ ¨ μµμ ν™”  
- **4bit μ–‘μν™”**: QLoRAλ¥Ό ν†µν• λ©”λ¨λ¦¬ μ μ•½
- **μλ™ μ²΄ν¬ν¬μΈνΈ**: ν›λ ¨ μ¤‘λ‹¨ μ‹ μ¬μ‹μ‘ κ°€λ¥

---

## π“¦ μ”κµ¬μ‚¬ν•­

```bash
pip install torch transformers datasets peft accelerate deepspeed bitsandbytes wandb
```

---

## π“„ λ°μ΄ν„°μ…‹ ν•μ‹

λ°μ΄ν„°μ…‹μ€ λ‹¤μ ν•μ‹μ΄μ–΄μ•Ό ν•©λ‹λ‹¤:

```json
{
  "input": "μ§λ¬Έ ν…μ¤νΈ",
  "output": "λ‹µλ³€ ν…μ¤νΈ", 
  "source": "λ°μ΄ν„° μ¶μ² (μ„ νƒμ‚¬ν•­)"
}
```

---

## β™οΈ μ‚¬μ©λ²•

### 1. μ„¤μ • μμ •

`shell_script.sh`μ—μ„ κ²½λ΅μ™€ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μμ •ν•μ„Έμ”:

```bash
# λ¨λΈ λ° λ°μ΄ν„° κ²½λ΅
BASE_MODEL_NAME="/path/to/your/base/model"
DATA_PATH="/path/to/your/dataset"
CHECKPOINT_PATH="/path/to/checkpoint"
BASE_SAVE_PATH="/path/to/save/model"

# ν•μ΄νΌνλΌλ―Έν„°
USE_QLORA="True"  # QLoRA μ‚¬μ© μ—¬λ¶€
NUM_EPOCHS=3      # ν›λ ¨ μ—ν­ μ
LORA_RANK=16      # LoRA λ­ν¬
```

---

### 2. ν›λ ¨ μ‹¤ν–‰

```bash
chmod +x run.sh
./run.sh
```

---

### 3. ν›λ ¨ μ¬μ‹μ‘ (μµμ…)

μ²΄ν¬ν¬μΈνΈμ—μ„ ν›λ ¨μ„ μ¬μ‹μ‘ν•λ ¤λ©΄:

```bash
# run.shμ—μ„ JOB λ³€μ μμ •
JOB="training"
```

---

## β™οΈ μ£Όμ” νλΌλ―Έν„°

| νλΌλ―Έν„°        | μ„¤λ…                           | κΈ°λ³Έκ°’                     |
|------------------|----------------------------------|-----------------------------|
| `USE_QLORA`      | QLoRA μ‚¬μ© μ—¬λ¶€                  | `True`                      |
| `NUM_EPOCHS`     | ν›λ ¨ μ—ν­ μ                     | `3`                         |
| `LORA_RANK`      | LoRA λ­ν¬ (λ¨λΈ λ³µμ΅λ„)         | `16`                        |
| `LORA_ALPHA`     | LoRA μ¤μΌ€μΌλ§ νλΌλ―Έν„°           | `32`                        |
| `LEARNING_RATE`  | ν•™μµλ¥                             | `1e-4` (QLoRA), `5e-6` (LoRA) |

---

## π§  λ©”λ¨λ¦¬ μµμ ν™” μ „λµ

- **QLoRA**: 4bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ†  
- **Gradient Checkpointing**: λ©”λ¨λ¦¬ β†” κ³„μ‚° νΈλ μ΄λ“μ¤ν”„  
- **DeepSpeed**: λ¶„μ‚° ν›λ ¨μΌλ΅ λ©”λ¨λ¦¬ λ¶„μ‚°  

---

## π“ λ¨λ‹ν„°λ§

ν›λ ¨ μ§„ν–‰ μƒν™©μ€ **Weights & Biases (wandb)**λ΅ λ¨λ‹ν„°λ§λ©λ‹λ‹¤:

- **ν”„λ΅μ νΈλ…**: `RAPA`  
- **μ‹¤ν–‰λ…**: `{λ¨λΈλ…}_{LoRAμ ν•}`  

---

## π“ μ¶λ ¥ κµ¬μ΅°

```plaintext
final_model/
β”β”€β”€ {model_name}/
β”‚   β””β”€β”€ {num_epochs}/
β”‚       β”β”€β”€ qlora_adapters/  # QLoRA μ–΄λ‘ν„°
β”‚       β””β”€β”€ lora_adapters/   # LoRA μ–΄λ‘ν„°
```