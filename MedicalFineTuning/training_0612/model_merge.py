import argparse
import os
import torch
from transformers import (
    BitsAndBytesConfig,
    AutoModelForCausalLM,
    AutoTokenizer
)
from peft import PeftModel

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë¹„í™œì„±í™”
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

def str2bool(v):
    """
    argparseì—ì„œ bool íƒ€ì… ì¸ìë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
    """
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def main():
    """
    LoRA/QLoRA ì–´ëŒ‘í„°ì™€ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë³‘í•©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
    """
    parser = argparse.ArgumentParser()
    # -------------------- ì¸ì ì •ì˜ --------------------
    parser.add_argument("--base_model_name", type=str, default="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
    parser.add_argument("--save_path", type=str, default="/home/project/rapa/final_model")
    parser.add_argument("--use_qlora", type=str2bool, default=True)
    parser.add_argument("--torch_dtype_bf16", type=str2bool, default=True)
    parser.add_argument("--num_epochs", type=int, default=2)

    args = parser.parse_args()

    print("ğŸš€ ëª¨ë¸ ë³‘í•© ì‹œì‘")

    # -------------------- ë² ì´ìŠ¤ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ --------------------
    model_path = args.base_model_name

    quantization_config = None
    if args.use_qlora:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

    print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config if args.use_qlora else None,
        torch_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
        trust_remote_code=True,
    )

    model_short_name = args.base_model_name
    if '/home' in model_short_name:
        model_short_name = model_short_name.split('/')[-1]
    save_path = f'{args.save_path}/{model_short_name}'

    adapter_suffix = "qlora_adapters" if args.use_qlora else "lora_adapters"
    adapter_output_dir = f"{save_path}/{args.num_epochs}/{adapter_suffix}"
    merged_suffix = "qlora_merged" if args.use_qlora else "lora_merged"
    merged_output_dir = f"{save_path}/{args.num_epochs}/{merged_suffix}"

    # -------------------- ì–´ëŒ‘í„° ë³‘í•© ë° ì €ì¥ --------------------
    print("ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©...")
    model = PeftModel.from_pretrained(model, adapter_output_dir)
    model = model.merge_and_unload()

    print("ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥...")
    model.save_pretrained(merged_output_dir)

    # í† í¬ë‚˜ì´ì € ì €ì¥
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer.save_pretrained(merged_output_dir)

    print(f"âœ… ì™„ë£Œ: {merged_output_dir}")

if __name__ == "__main__":
    main()
