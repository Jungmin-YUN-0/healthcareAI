import argparse
import os
import re
import torch
from datasets import load_from_disk
from transformers import (
    BitsAndBytesConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from accelerate import Accelerator

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë¹„í™œì„±í™”
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

def str2bool(v):
    """
    argparseì—ì„œ bool íƒ€ì… ì¸ìë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
    """
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def prompt_generate(example):
    """
    ë°ì´í„°ì…‹ì˜ input/outputì„ í”„ë¡¬í”„íŠ¸ í˜•íƒœì˜ textë¡œ ë³€í™˜
    """
    input_text = example['input']
    output_text = example['output']
    
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {input_text}\në‹µë³€: {output_text}"
    
    return {"text": prompt}

def tokenize_function(examples, tokenizer, max_length):
    """
    í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³ ,
    í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸) ë¶€ë¶„ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ ë¼ë²¨ ìƒì„±
    """
    # ì „ì²´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (íŒ¨ë”© í¬í•¨)
    tokenized = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors=None,
        add_special_tokens=True
    )
    
    # labels ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    labels = []
    
    # ê° ì˜ˆì œì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹
    for i, text in enumerate(examples["text"]):
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì¶”ì¶œ (ì§ˆë¬¸ ë¶€ë¶„)
        if "\në‹µë³€: " in text:
            prompt_part = text.split("\në‹µë³€: ")[0] + "\në‹µë³€: "
        else:
            prompt_part = text.split("\n")[0] + "\n"
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ í† í¬ë‚˜ì´ì§• (ê¸¸ì´ ê³„ì‚°ìš©)
        prompt_tokens = tokenizer(
            prompt_part,
            add_special_tokens=True,
            return_tensors=None
        )
        prompt_length = len(prompt_tokens["input_ids"])
        
        # ì „ì²´ input_idsì—ì„œ labels ìƒì„± (ë³µì‚¬)
        example_labels = tokenized["input_ids"][i][:]
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        for j in range(min(prompt_length, len(example_labels))):
            example_labels[j] = -100
            
        # íŒ¨ë”© í† í°ë„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        for j in range(len(example_labels)):
            if tokenized["input_ids"][i][j] == tokenizer.pad_token_id:
                example_labels[j] = -100
                
        labels.append(example_labels)
    
    # tokenized ê²°ê³¼ì— labels ì¶”ê°€
    tokenized["labels"] = labels
    
    return tokenized

def main():
    """
    í•™ìŠµ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    - ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    - ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    - LoRA/QLoRA ì„¤ì • ë° ì ìš©
    - Trainerë¥¼ í†µí•œ í•™ìŠµ ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
    - ì–´ëŒ‘í„° ë° í† í¬ë‚˜ì´ì € ì €ì¥
    """
    parser = argparse.ArgumentParser()
    # -------------------- ì¸ì ì •ì˜ --------------------
    # ëª¨ë¸ ë° ë°ì´í„° ê²½ë¡œ ì¸ì
    parser.add_argument("--base_model_name", type=str, default="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
    parser.add_argument("--save_path", type=str, default="/home/project/rapa/final_model")
    parser.add_argument("--checkpoint_path", type=str, default="/home/project/rapa/checkpoint")
    parser.add_argument("--data_path", type=str, default="/home/project/rapa/dataset/dataset_fintuning_0603")
    
    # Deepspeed ê´€ë ¨ ì¸ì
    parser.add_argument("--deepspeed_config_path", type=str, default='./ds_config')
    parser.add_argument("--use_deepspeed", type=bool, default=False)
    parser.add_argument("--local_rank", type=int, default=-1)
    
    # LoRA/QLoRA ê´€ë ¨ ì¸ì
    parser.add_argument("--lora_rank", type=int, default=16)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    parser.add_argument("--use_qlora", type=str2bool, default=True)
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--job', type=str, default='training', choices=['training', 'resume_training'])
    parser.add_argument("--learning_rate", type=float, default=5e-6)
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.99)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_ratio", type=float, default=0.1)
    parser.add_argument("--lr_scheduler_type", type=str, default="cosine")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--num_generations", type=int, default=4)
    parser.add_argument("--max_grad_norm", type=float, default=0.1)
    parser.add_argument("--num_epochs", type=int, default=2)
    parser.add_argument("--max_steps", type=int, default=-1)
    parser.add_argument("--save_steps", type=int, default=500)
    
    # í•™ìŠµ ê¸¸ì´ ê´€ë ¨ ì¸ì
    parser.add_argument("--max_length", type=int, default=512)
    
    # ê¸°íƒ€ ì¸ì
    parser.add_argument("--random_seed", type=int, default=42)
    parser.add_argument("--torch_dtype_bf16", type=str2bool, default=True)
    parser.add_argument("--use_gradient_checkpointing", type=str2bool, default=True)
    
    args = parser.parse_args()
    
    # WANDB ì„¤ì •
    os.environ["WANDB_PROJECT"] = 'RAPA'
    os.environ["WANDB_DISABLED"] = "false"
    
    # ëœë¤ ì‹œë“œ ê³ ì •
    torch.manual_seed(args.random_seed)
    if args.use_deepspeed:
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        device = torch.device(f"cuda:{local_rank}")

    # -------------------- ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ --------------------
    dataset = load_from_disk(args.data_path)
    train_dataset = dataset['train']
    
    # í”„ë¡¬í”„íŠ¸ í˜•íƒœë¡œ ë³€í™˜
    train_dataset = train_dataset.map(
        prompt_generate, 
        remove_columns=['input', 'output', 'source'],
        cache_file_name=None,
        load_from_cache_file=False
    )
    
    # -------------------- í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ --------------------
    quantization_config = None
    if args.use_qlora:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    model_path = args.base_model_name
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    if (tokenizer.pad_token is None) and (tokenizer.eos_token is not None):
        tokenizer.pad_token = tokenizer.eos_token
    
    # í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ë§ˆìŠ¤í‚¹
    train_dataset = train_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer, args.max_length),
        batched=True,
        remove_columns=['text'],
        cache_file_name=None,
        load_from_cache_file=False,
        desc="í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ë§ˆìŠ¤í‚¹"
    )
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config if args.use_qlora else None,
        torch_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
        trust_remote_code=True,    
    )

    # -------------------- LoRA/QLoRA ì ìš© --------------------
    if args.use_gradient_checkpointing:
        model.gradient_checkpointing_enable()

    if args.use_qlora:
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.use_gradient_checkpointing)
    
    lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # -------------------- Trainer ë° í•™ìŠµ --------------------
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8,
    )
    
    # ëŸ¬ë‹ ì´ë¦„ ë° ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
    model_short_name = args.base_model_name
    if '/home' in model_short_name:
        model_short_name = model_short_name.split('/')[-1] 
    lora_type = "QLoRA" if args.use_qlora else "LoRA"
    run_name = f"RAPA/{model_short_name}_{lora_type}"
    
    if args.use_qlora:
        ds_config = f'{args.deepspeed_config_path}_stage2.json'
        checkpoint_path = f'{args.checkpoint_path}/{model_short_name}/qlora'

    else:
        ds_config = f'{args.deepspeed_config_path}_stage3.json'
        checkpoint_path = f'{args.checkpoint_path}/{model_short_name}/lora'
    
    training_args = TrainingArguments(
        max_steps=10,
        output_dir=checkpoint_path,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        save_strategy="epoch",                     # ë§¤ epoch ëë§ˆë‹¤ ì €ì¥
        logging_steps=500,
        bf16=args.torch_dtype_bf16,
        remove_unused_columns=False,
        report_to='wandb',
        run_name=run_name,
        do_train=True,
        deepspeed=ds_config if args.use_deepspeed else None,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # -------------------- í•™ìŠµ ì‹œì‘ ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ --------------------
    if args.job == 'resume_training':
        checkpoint_root = checkpoint_path

        if os.path.exists(checkpoint_root):
            checkpoint_dirs = [d for d in os.listdir(checkpoint_root) if re.match(r'checkpoint-\d+', d)]
            if checkpoint_dirs:
                latest_checkpoint = max(
                    checkpoint_dirs,
                    key=lambda x: int(x.split('-')[-1])
                )
                resume_path = os.path.join(checkpoint_root, latest_checkpoint)
                print(f"ğŸ” ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {resume_path}")
                trainer.train(resume_from_checkpoint=resume_path)
            else:
                print("âŒ checkpoint ë””ë ‰í† ë¦¬ì—ëŠ” checkpoint-* í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                trainer.train()
        else:
            print("âŒ checkpoint ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            trainer.train()
    else:
        trainer.train()
    print('Train Finish')
    
    # -------------------- ì–´ëŒ‘í„° ë° í† í¬ë‚˜ì´ì € ì €ì¥ --------------------
    save_path = f'{args.save_path}/{model_short_name}'
    
    if save_path:
        adapter_suffix = "qlora_adapters" if args.use_qlora else "lora_adapters"
        adapter_output_dir = f"{save_path}/{args.num_epochs}/{adapter_suffix}"
        trainer.save_model(adapter_output_dir)
        tokenizer.save_pretrained(adapter_output_dir)
        print(f"ì–´ëŒ‘í„°ì™€ í† í¬ë‚˜ì´ì €ê°€ {adapter_output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
