import argparse
import torch
import os
import re
from datasets import load_from_disk
from transformers import (
    BitsAndBytesConfig, 
    AutoModelForCausalLM, 
    AutoTokenizer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    Trainer,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë¹„í™œì„±í™”
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def get_model_config(model_name):
    """
    ëª¨ë¸ë³„ ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    model_configs = {
        "yanolja/EEVE-Korean-10.8B-v1.0": {
            "prompt_template": "Human: {prompt}\nAssistant: {response}",
            "stop_criteria": ["Human:", "\n\nHuman:", "Assistant:", "\n\nAssistant:"],
            "model_type": "eeve"
        },
        "trillionlabs/Tri-7B": {
            "prompt_template": "<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|>",
            "stop_criteria": ["<|im_start|>", "<|im_end|>", "<|endoftext|>"],
            "model_type": "tri"
        },
        "skt/A.X-4.0-Light": {
            "prompt_template": "<s>[INST] {prompt} [/INST] {response}</s>",
            "stop_criteria": ["[INST]", "[/INST]", "<s>", "</s>"],
            "model_type": "ax"
        }
    }
    
    for model_key in model_configs:
        if model_key in model_name or model_name in model_key:
            return model_configs[model_key]
    
    # Default configuration for other models
    return {
        "prompt_template": "ì§ˆë¬¸: {prompt}\në‹µë³€: {response}",
        "stop_criteria": ["ì§ˆë¬¸:", "\nì§ˆë¬¸:", "ë‹µë³€:", "\në‹µë³€:"],
        "model_type": "default"
    }

def prompt_generate(example, system_prompt=None, use_system_prompt=False, output_key='output', model_name=None):
    """
    ë°ì´í„°ì…‹ì„ í”„ë¡¬í”„íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    input_text = example['input']
    output_text = example[output_key]
    
    # ëª¨ë¸ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = get_model_config(model_name) if model_name else get_model_config("default")
    
    if use_system_prompt and system_prompt:
        if model_config["model_type"] == "eeve":
            prompt = f"{system_prompt}\n\nHuman: {input_text}\nAssistant: {output_text}"
        elif model_config["model_type"] == "tri":
            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        elif model_config["model_type"] == "ax":
            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        else:
            prompt = f"{system_prompt}\n\nì§ˆë¬¸: {input_text}\në‹µë³€: {output_text}"
    else:
        if model_config["model_type"] == "eeve":
            prompt = f"Human: {input_text}\nAssistant: {output_text}"
        elif model_config["model_type"] == "tri":
            prompt = f"<|im_start|>system\nYou are Trillion, created by TrillionLabs. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        elif model_config["model_type"] == "ax":
            prompt = f"<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        else:
            prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {input_text}\në‹µë³€: {output_text}"
    
    return {"text": prompt}


def tokenize_function(examples, tokenizer, max_length, model_name=None):
    """
    í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³ ,
    í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸) ë¶€ë¶„ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ ë¼ë²¨ ìƒì„±
    """
    # ì „ì²´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (íŒ¨ë”© í¬í•¨)
    tokenized = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors=None,
        add_special_tokens=True
    )
    
    # labels ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    labels = []
    
    # ê° ì˜ˆì œì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹
    model_config = get_model_config(model_name) if model_name else get_model_config("default")
    
    for i, text in enumerate(examples["text"]):
        # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì¶”ì¶œ
        if model_config["model_type"] == "eeve":
            if "\nAssistant: " in text:
                prompt_part = text.split("\nAssistant: ")[0] + "\nAssistant: "
            else:
                prompt_part = text.split("\n")[0] + "\n"
        elif model_config["model_type"] == "tri":
            if "<|im_start|>assistant\n" in text:
                prompt_part = text.split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
            else:
                # systemê³¼ user ë¶€ë¶„ê¹Œì§€ í¬í•¨í•´ì„œ ë§ˆìŠ¤í‚¹
                if "<|im_start|>system" in text and "<|im_start|>user" in text:
                    prompt_part = text.split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
                else:
                    prompt_part = text.split("<|im_end|>")[0] + "<|im_end|>\n"
        elif model_config["model_type"] == "ax":
            if "<|im_start|>assistant\n" in text:
                prompt_part = text.split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
            else:
                if "<|im_start|>system" in text and "<|im_start|>user" in text:
                    prompt_part = text.split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
                else:
                    prompt_part = text.split("<|im_end|>")[0] + "<|im_end|>\n"
        else:
            # Default behavior
            if "\në‹µë³€: " in text:
                prompt_part = text.split("\në‹µë³€: ")[0] + "\në‹µë³€: "
            else:
                prompt_part = text.split("\n")[0] + "\n"
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ í† í¬ë‚˜ì´ì§• (ê¸¸ì´ ê³„ì‚°ìš©)
        prompt_tokens = tokenizer(
            prompt_part,
            add_special_tokens=True,
            return_tensors=None
        )
        prompt_length = len(prompt_tokens["input_ids"])
        
        # ì „ì²´ input_idsì—ì„œ labels ìƒì„± (ë³µì‚¬)
        example_labels = tokenized["input_ids"][i].copy()
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        for j in range(min(prompt_length, len(example_labels))):
            example_labels[j] = -100
            
        # íŒ¨ë”© í† í°ë„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        for j in range(len(example_labels)):
            if tokenized["input_ids"][i][j] == tokenizer.pad_token_id:
                example_labels[j] = -100
                
        labels.append(example_labels)
    # tokenized ê²°ê³¼ì— labels ì¶”ê°€
    tokenized["labels"] = labels
    
    return tokenized

def main():
    parser = argparse.ArgumentParser()
    
    # Model arguments
    parser.add_argument("--base_model_name", type=str, default="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
    parser.add_argument("--save_path", type=str, default="/home/project/rapa/final_model")
    parser.add_argument("--cache_path", type=str, default="/home/project/rapa/cache")
    parser.add_argument("--checkpoint_path", type=str, default="/home/project/rapa/checkpoint")
    parser.add_argument("--data_path", type=str, default="/home/project/rapa/dataset/dataset_fintuning_0603")
    
    # Deepspeed arguments
    parser.add_argument("--deepspeed_config_path", type=str, default='./ds_config')
    parser.add_argument("--use_deepspeed", type=bool, default=False)
    parser.add_argument("--local_rank", type=int, default=-1)
    
    # Training arguments - LoRA & QLoRA
    parser.add_argument("--lora_rank", type=int, default=16)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    parser.add_argument("--use_qlora", type=str2bool, default=True)
    
    # Training arguments - Hyperparameters
    parser.add_argument('--job', type=str, default='training', choices=['training', 'resume_training'])
    parser.add_argument("--learning_rate", type=float, default=5e-6)
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.99)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_ratio", type=float, default=0.1)
    parser.add_argument("--lr_scheduler_type", type=str, default="cosine")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2)
    parser.add_argument("--per_device_train_batch_size", type=int, default=2)
    parser.add_argument("--num_generations", type=int, default=4)
    parser.add_argument("--max_grad_norm", type=float, default=0.1)
    parser.add_argument("--num_epochs", type=int, default=2)
    parser.add_argument("--max_steps", type=int, default=-1)
    parser.add_argument("--save_steps", type=int, default=500)
    
    # Training arguments - Generation length
    parser.add_argument("--max_length", type=int, default=384)
    
    
    # Type arguments
    parser.add_argument("--type", type=int, choices=[1, 2, 3], required=True,
                       help="Training type: 1 (structured), 2 (shortened), 3 (shortened_50)")
    
    # Other arguments
    parser.add_argument("--random_seed", type=int, default=42)
    parser.add_argument("--torch_dtype_bf16", type=str2bool, default=True)
    parser.add_argument("--use_gradient_checkpointing", type=str2bool, default=True)
    
    args = parser.parse_args()
    
    os.environ["WANDB_PROJECT"] = 'RAPA'
    os.environ["WANDB_DISABLED"] = "false"
    
    # Set random seed
    torch.manual_seed(args.random_seed)
    if args.use_deepspeed:
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        device = torch.device(f"cuda:{local_rank}")
    # Load dataset
    dataset = load_from_disk(args.data_path)
    train_dataset = dataset['train'].shuffle(seed=args.random_seed)
    
    # Define system prompts based on type
    system_prompts = {
        1: '''## ì‘ë‹µ ì¶œë ¥ ì§€ì¹¨
- ì¶œë ¥ í˜•ì‹: ì•„ë˜ JSON í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¶”ê°€ í…ìŠ¤íŠ¸/ì„¤ëª…/ê¸°í˜¸ ê¸ˆì§€)
{"_original": "ì •ì‹ ë‹µë³€", "_short": "ê°„ë‹¨ ìš”ì•½ ë¬¸ì¥"}
- _original: **ë°˜ë“œì‹œ 3ë¬¸ì¥ ì´í•˜, 70ì ì´ë‚´**ì˜ ê°„ê²°í•œ ë‹µë³€
- _short: **2ë¬¸ì¥ ì´í•˜, 20ì ì´ë‚´, í•µì‹¬ ë‚´ìš© ìœ„ì£¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ ìš”ì•½** (ê°œì¡°ì‹ ê¸ˆì§€)
ì˜ˆì‹œ:
{"_original":"ì½œë ˆìŠ¤í…Œë¡¤ ê´€ë¦¬ê°€ ì¤‘ìš”í•´ìš”. ìš´ë™ê³¼ ì‹ë‹¨ì„ ì‹ ê²½ ì¨ë³´ì„¸ìš”. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”?","_short":"ì½œë ˆìŠ¤í…Œë¡¤ ê´€ë¦¬ê°€ ì¤‘ìš”í•´ìš”."}

## ì—­í• : ê±´ê°• ì „ë¬¸ ìƒë‹´ì‚¬ AI
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì´ë©°, ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆëŠ” ê±´ê°• ì •ë³´ ì œê³µ
- ì§„ë‹¨ì´ë‚˜ ì¹˜ë£ŒëŠ” í•˜ì§€ ì•Šìœ¼ë©°, í•„ìš” ì‹œ ì˜ë£Œ ì „ë¬¸ê°€ë‚˜ ì§„ë£Œê³¼ ì•ˆë‚´

## ë‹µë³€ ì§€ì¹¨
1. ê¸¸ì´ ì œí•œ
- ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ **3ë¬¸ì¥ ì´í•˜, 70ì ì´ë‚´**ë¡œ ì œí•œ
- ë¶€ì—°ì„¤ëª… ì—†ì´, ì§§ê³  ëª…í™•í•˜ê²Œ í•µì‹¬ ì •ë³´ë§Œ ì „ë‹¬
2. ëŒ€í™” ìœ ë„ ë° ì¶”ê°€ ì§ˆë¬¸
- ì§§ì€ ì‘ë‹µ í›„, ê´€ë ¨ í›„ì† ì§ˆë¬¸ ë˜ëŠ” ì¦ìƒ êµ¬ì²´í™” ì§ˆë¬¸ ì œì‹œ
- ì˜ˆ: "ì–¸ì œë¶€í„° ì´ëŸ° ì¦ìƒì´ ìˆì—ˆë‚˜ìš”?"
3. ì „ë¬¸ì˜ ì•ˆë‚´
- í•„ìš”í•œ ê²½ìš°ì— í•œí•´, ì˜ì‹¬ ì¦ìƒì— ë”°ë¼ í•´ë‹¹ ì§„ë£Œ ê³¼ ê¶Œìœ 
- ì˜ˆ: "ì´ëŸ° ì¦ìƒì€ ë‚´ê³¼ ì§„ë£Œê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”."
4. ë²”ìœ„ ì œí•œ
- ê±´ê°•ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì€ ì •ì¤‘íˆ ë¶ˆê°€ ì•ˆë‚´ í›„ ê±´ê°• ì£¼ì œë¡œ ìœ ë„
- ì˜ˆ: "ê·¸ ë¶€ë¶„ì€ ë‹µë³€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”. ê±´ê°• ê´€ë ¨ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”?"
5. í•œêµ­ì–´ ì‚¬ìš©
- í•­ìƒ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ ì‚¬ìš©
- ìš”ì²­ ì—†ëŠ” í•œ ì˜ì–´ ë° ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ê¸ˆì§€''',
        2: '''## ì‘ë‹µ ì¶œë ¥ ì§€ì¹¨
- ì¶œë ¥ í˜•ì‹: ì•„ë˜ JSON í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¶”ê°€ í…ìŠ¤íŠ¸/ì„¤ëª…/ê¸°í˜¸ ê¸ˆì§€)
{"_answer": "ì •ì‹ ë‹µë³€", "_follow_up": "ì¶”ê°€ ì§ˆë¬¸"}
- _answer: 3ë¬¸ì¥ ì´í•˜, ë°˜ë“œì‹œ **45ì ì´ë‚´** ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì ¸ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•˜ê²Œ ì˜ë¼ë‚´ê±°ë‚˜ ë”±ë”±í•˜ê²Œ í‘œí˜„í•˜ì§€ ì•ŠìŒ. (ê°œì¡°ì‹ ê¸ˆì§€)
- _follow_up: ë‹µë³€ íë¦„ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ í›„ì† ì§ˆë¬¸ í•œ ë¬¸ì¥

## ì—­í• : ê±´ê°• ì „ë¬¸ ìƒë‹´ì‚¬ AI
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì´ë©°, ì¼ìƒ ëŒ€í™”ì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì–´íˆ¬ë¡œ ê±´ê°• ì •ë³´ë¥¼ ì œê³µ
- ì „ë¬¸ ìš©ì–´ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•˜ê³ , ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ìš°ì„ 
- ì§„ë‹¨ì´ë‚˜ ì¹˜ë£ŒëŠ” í•˜ì§€ ì•Šìœ¼ë©°, í•„ìš” ì‹œ ê´€ë ¨ ì˜ë£Œ ì „ë¬¸ê°€ë‚˜ ì§„ë£Œê³¼ ì•ˆë‚´

## ë‹µë³€ ì§€ì¹¨
1. ê¸¸ì´ ì œí•œ
- _answer: 45ì ì´ë‚´, 3ë¬¸ì¥ ì´í•˜
- _follow_up: 1ë¬¸ì¥
- í•µì‹¬ ì •ë³´ ì „ë‹¬ì„ ìš°ì„ í•˜ë˜, ë¬¸ì¥ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²° ìœ ì§€
2. _answer ì‘ì„±
- í•µì‹¬ ë‚´ìš©ì„ ì§§ê³  ëª…í™•í•˜ê²Œ, ë¶€ë“œëŸ¬ìš´ ë¬¸ì¥ íë¦„ìœ¼ë¡œ ì‘ì„±
3. follow_up ì‘ì„±
- ë‹µë³€ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” í›„ì† ì§ˆë¬¸ ì œì‹œ
- ì˜ˆ: "ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?" / "ì¡°ê¸ˆ ë” ì„¤ëª…í•´ì£¼ì‹¤ë˜ìš”?" / "ì¢€ ë” ìì„¸íˆ ì„¤ëª…ë“œë¦´ê¹Œìš”?" ë“±
4. ì „ë¬¸ì˜ ì•ˆë‚´
- í•„ìš”í•œ ê²½ìš°, ì˜ì‹¬ ì¦ìƒì— ë”°ë¼ í•´ë‹¹ ì§„ë£Œ ê³¼ë¥¼ ê°„ë‹¨íˆ ì•ˆë‚´
- ì˜ˆ: "ë‚´ê³¼ ì§„ë£Œê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”."
5. ê±´ê°•ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì€ ì •ì¤‘íˆ ë¶ˆê°€ ì•ˆë‚´ í›„ ê±´ê°• ì£¼ì œë¡œ ìœ ë„
- ì˜ˆ: "ê·¸ ë¶€ë¶„ì€ ë‹µë³€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”. ê±´ê°• ê´€ë ¨ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”?"
6. ì–¸ì–´ ê·œì¹™
- í•­ìƒ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ ì‚¬ìš©
- ìš”ì²­ ì—†ëŠ” í•œ ì˜ì–´Â·ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ê¸ˆì§€''',
        3: '''## ì—­í• : ê±´ê°• ì „ë¬¸ ìƒë‹´ì‚¬ AI
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì´ë©´ì„œë„ ì¼ìƒ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ê±´ê°• ì •ë³´ë¥¼ ì œê³µ
- ì „ë¬¸ ìš©ì–´ëŠ” ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•˜ê³ , ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ìš°ì„ 
- ì§„ë‹¨ì´ë‚˜ ì¹˜ë£ŒëŠ” í•˜ì§€ ì•Šìœ¼ë©°, í•„ìš” ì‹œ ê´€ë ¨ ì˜ë£Œ ì „ë¬¸ê°€ë‚˜ ì§„ë£Œê³¼ ì•ˆë‚´

## ë‹µë³€ ì§€ì¹¨
- í•µì‹¬ ì •ë³´ë¥¼ 50ì ë‚´ì™¸ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ, ë¶€ë“œëŸ¬ìš´ íë¦„ìœ¼ë¡œ ì‘ì„±
- í•„ìš” ì‹œ ë‹µë³€ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” í›„ì† ì§ˆë¬¸ í¬í•¨
- ê±´ê°•ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì€ ì •ì¤‘íˆ ë¶ˆê°€ ì•ˆë‚´ í›„ ê±´ê°• ì£¼ì œë¡œ ìœ ë„
- ìš”ì²­ ì—†ëŠ” í•œ í•­ìƒ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ ì‚¬ìš©'''
    }
    
    # Set system prompt based on type
    system_prompt = system_prompts[args.type]
    print(f"ìœ í˜• {args.type}ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # Define output key based on type
    output_keys = {
        1: 'output_structured',
        2: 'output_shortened', 
        3: 'output_shortened_50'
    }
    output_key = output_keys[args.type]
    
    # Configure quantization if using QLoRA
    quantization_config = None
    if args.use_qlora:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    # Load model and tokenizer based on model name
    if args.base_model_name == "EEVE+ChatVector":
        model_name = 'yanolja/EEVE-Korean-10.8B-v1.0'
        model_path = "/home/project/rapa/ckpt/eeve_chatvector_IT_OB"
    elif args.base_model_name == 'eeve_inst_chatvector_OB':
        model_name = '/home/project/rapa/ckpt/eeve_inst_chatvector_OB'
        model_path = '/home/project/rapa/ckpt/eeve_inst_chatvector_OB'
    elif args.base_model_name == "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct":
        model_name = args.base_model_name
        model_path = args.base_model_name
    elif args.base_model_name == 'eeve_chatvector_OB_update':
        model_name = '/home/project/rapa/ckpt/eeve_chatvector_OB_update'
        model_path = '/home/project/rapa/ckpt/eeve_chatvector_OB_update'
    elif args.base_model_name == "yanolja/EEVE-Korean-10.8B-v1.0":
        model_name = 'yanolja/EEVE-Korean-10.8B-v1.0'
        model_path = "yanolja/EEVE-Korean-10.8B-v1.0"
    elif args.base_model_name == "trillionlabs/Tri-7B":
        model_name = 'trillionlabs/Tri-7B'
        model_path = "trillionlabs/Tri-7B"
    elif args.base_model_name == "skt/A.X-4.0-Light":
        model_name = 'skt/A.X-4.0-Light'
        model_path = "skt/A.X-4.0-Light"
    else:
        model_name = args.base_model_name
        model_path = args.base_model_name
    
    # ë°ì´í„°ì…‹ì„ í”„ë¡¬í”„íŠ¸ í˜•íƒœë¡œ ë³€í™˜
    columns_to_keep = ['input', output_key]
    columns_to_remove = [col for col in train_dataset.column_names if col not in columns_to_keep]
    
    train_dataset = train_dataset.map(
        lambda examples: prompt_generate(examples, system_prompt, True, output_key, model_name), 
        remove_columns=columns_to_remove,
        cache_file_name=None,
        load_from_cache_file=False
    )
    
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=args.cache_path,
        trust_remote_code=True
    )
    
    
    if (tokenizer.pad_token is None) and (tokenizer.eos_token is not None):
        tokenizer.pad_token = tokenizer.eos_token
    
    # í† í¬ë‚˜ì´ì§• ì ìš© (íŒ¨ë”©ê¹Œì§€ ë¯¸ë¦¬ ì ìš©)
    train_dataset = train_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer, args.max_length, model_name),
        batched=True,
        remove_columns=['text', 'input', output_key], 
        desc="í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ë§ˆìŠ¤í‚¹"
    )
    
    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config if args.use_qlora else None,
        torch_dtype=torch.bfloat16 if args.torch_dtype_bf16 else torch.float16,
        trust_remote_code=True,    
        cache_dir=args.cache_path,
    )
    
    # Prepare model for training
    if args.use_gradient_checkpointing:
        model.gradient_checkpointing_enable()

    if args.use_qlora:
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.use_gradient_checkpointing)
    
    # Configure LoRA
    lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # Apply LoRA to model
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Generate run name based on model, LoRA type, and training type
    model_short_name = args.base_model_name
    lora_type = "QLoRA" if args.use_qlora else "LoRA"
    type_suffix = f"_v{args.type}"
    run_name = f"RAPA/{model_short_name}_{lora_type}{type_suffix}"
    
    if args.use_qlora:
        ds_config = f'{args.deepspeed_config_path}_stage2.json'
        checkpoint_path = f'{args.checkpoint_path}/{args.base_model_name}/qlora{type_suffix}'
    else:
        ds_config = f'{args.deepspeed_config_path}_stage3.json'
        checkpoint_path = f'{args.checkpoint_path}/{args.base_model_name}/lora{type_suffix}'
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8,
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=checkpoint_path,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        save_strategy="epoch",                     # ë§¤ epoch ëë§ˆë‹¤ ì €ì¥
        logging_steps=500,
        bf16=args.torch_dtype_bf16,
        remove_unused_columns=False,
        report_to='wandb',
        run_name=run_name,
        do_train=True,
        deepspeed=ds_config if args.use_deepspeed else None,
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    # Start training
    if args.job == 'resume_training':
        checkpoint_root = checkpoint_path

        if os.path.exists(checkpoint_root):
            # checkpoint-* ë””ë ‰í† ë¦¬ë“¤ ì¤‘ì—ì„œ ìˆ«ì ê°€ì¥ í° ê±° ì°¾ê¸°
            checkpoint_dirs = [d for d in os.listdir(checkpoint_root) if re.match(r'checkpoint-\d+', d)]
            if checkpoint_dirs:
                latest_checkpoint = max(
                    checkpoint_dirs,
                    key=lambda x: int(x.split('-')[-1])
                )
                resume_path = os.path.join(checkpoint_root, latest_checkpoint)
                print(f"ğŸ” ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {resume_path}")
                trainer.train(resume_from_checkpoint=resume_path)
            else:
                print("âŒ checkpoint ë””ë ‰í† ë¦¬ì—ëŠ” checkpoint-* í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                trainer.train()
        else:
            print("âŒ checkpoint ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            trainer.train()
    else:
        trainer.train()
    print('Train Finish')
    save_path = f'{args.save_path}/{args.base_model_name}'
    # Save LoRA adapters
    if save_path:
        # use_qlora ê°’ì— ë”°ë¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
        adapter_suffix = "qlora_adapters" if args.use_qlora else "lora_adapters"
        adapter_output_dir = f"{save_path}/{args.num_epochs}/{adapter_suffix}{type_suffix}"
        # 1. ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ì‘ì€ ìš©ëŸ‰)
        trainer.save_model(adapter_output_dir)
        tokenizer.save_pretrained(adapter_output_dir)
        print(f"ì–´ëŒ‘í„°ì™€ í† í¬ë‚˜ì´ì €ê°€ {adapter_output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

 
if __name__ == "__main__":
    main()
