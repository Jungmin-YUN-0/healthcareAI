# %%
from datasets import load_from_disk
import torch
import matplotlib.pyplot as plt
from datasets import DatasetDict
from transformers import AutoTokenizer
import wandb
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import AdaLoraConfig, get_peft_model, TaskType
from datasets import DatasetDict


model_name = "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct"

# ì˜ˆì‹œ tokenizer (í•„ìš”ì— ë”°ë¼ êµì²´)
tokenizer = AutoTokenizer.from_pretrained(model_name)  # ë˜ëŠ” ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½



# -----------------------------
# ğŸŸ¡ 0. WandB ì„¤ì •
# -----------------------------
wandb_project_name = "exaone-adalora-finetune"  # â† ì—¬ê¸°ë§Œ ë„¤ê°€ ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ë©´ ë¼
wandb_run_name = "EXAONE-output-only-run"       # ì„ íƒ ì‚¬í•­
wandb.init(project=wandb_project_name, name=wandb_run_name)

# -----------------------------
# ğŸŸ¡ 1. Load model/tokenizer
# -----------------------------
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True
)


# %%
# -----------------------------
# ğŸŸ¡ 2. PEFT - AdaLoRA
# -----------------------------

# %%
tokenized_datasets = load_from_disk('/nas_homes/projects/rapa/Dataset/filtered_updated_datasets_512_instruction')

train_size = len(tokenized_datasets["train"])
batch_size = 4
grad_accum = 4
epochs = 5
total_step = (train_size // (batch_size * grad_accum)) * epochs

peft_config = AdaLoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    init_r=8,
    target_modules=["q_proj", "v_proj"],  # ê°€ì¥ ì˜í–¥ í° projection ëª¨ë“ˆë§Œ
    lora_alpha=32,
    lora_dropout=0.1,
    tinit=200,
    tfinal=1000,
    deltaT=10,
    beta1=0.85,
    beta2=0.85,
    total_step=total_step,  # ğŸ‘ˆ ë°˜ë“œì‹œ ì¶”ê°€!
)


model = get_peft_model(model, peft_config)



# %%
from transformers import default_data_collator

# -----------------------------
# ğŸŸ¡ 5. TrainingArguments (WandB ì„¤ì • í¬í•¨)
# -----------------------------
training_args = TrainingArguments(
    output_dir="exaone_adalora_ft",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=grad_accum,
    num_train_epochs=epochs,

    evaluation_strategy="epoch",      # âœ… epochë§ˆë‹¤ í‰ê°€
    save_strategy="epoch",            # âœ… epochë§ˆë‹¤ ì €ì¥
    save_total_limit = 1,
    logging_steps=1,                  # ê³„ì† ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ìœ ì§€
    learning_rate=5e-5,
    bf16=True,
    fp16=False,
    torch_compile=False,
    logging_dir="logs",
    report_to="wandb",
    run_name=wandb_run_name,
    deepspeed="ds_config.json",
)


# -----------------------------
# ğŸŸ¡ 6. Trainer + í•™ìŠµ ì‹¤í–‰
# -----------------------------
data_collator = default_data_collator

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

model.merge_and_unload()
output_dir = "/nas_homes/projects/rapa/Models/exaone_adalora_ft" # nas ëª¨ë¸ ì €ì¥ ê²½ë¡œ
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"âœ… Merged full model saved at {output_dir}")